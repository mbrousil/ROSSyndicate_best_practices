[["index.html", "ROSSyndicate Project and Workflow Best Practices Section 1 About 1.1 Overarching code/workflow philosophies of the ROSSyndicate 1.2 Contents 1.3 A living document 1.4 Origin", " ROSSyndicate Project and Workflow Best Practices ROSSyndicate 2024-02-14 Section 1 About This guide outlines the ROSSyndicate’s project management expectations and workflow best practices so that we can successfully implement Radical Open Science. 1.1 Overarching code/workflow philosophies of the ROSSyndicate We value literate, readable code without sacrificing reproducibility. This means deploying understandable code and repositories that are accompanied by narrative comments and text. We value transparency in our analyses and believe that well documented repositories and code facilitate that. All code and software are licensed with the highly permissive MIT use license. While we believe that all code and software should be public, we acknowledge that other researchers and users may wish to keep products derived from our code private. We intentionally avoid the GNU-GPL family of licenses that are “viral” by nature, forcing all downstream repositories to also be fully open. We do so because we acknowledge their are vulnerable populations that may want to keep their code private for reasons we may not understand. We hope that this encourages use of our code, and ultimately, creates an environment of sharing with and empowering each other. GitHub contributions follow the ‘fork’ methodology that is common in open science repositories (as opposed to the ‘branch’ method). All GitHub contributions are reviewed before being incorporated into the lab organization repository, except upon initial repository set up. These reviews are completed internally through pull requests. We value ‘bite-sized’ pull requests that usually only contain 500 lines of code. No code is formally published unless it has been consistently and intentionally reviewed. We make an effort to include external review of our repositories prior to formal publishing. We follow the {tidyverse} style guide. We use the {styler} Addin in RStudio to assist in linting our code. We strive use a set of common styles specific to the look and feel of rendered documents, which include: color palettes, ggthemes, and style.css files. These are meant to reduce the amount of time that researchers in this lab spend on relatively inconsequential decisions. Ideally the use of these aesthetics also creates a common look and feel of all deliverables from the ROSSyndicate. 1.2 Contents This best practices guide serves as an introduction to Git and GitHub, the lab’s style and code philosophies, and the tools we use for project management. 1.3 A living document The workflow best practices document is a living document and all members of the ROSSyndicate are welcome to submit edits to it via pull request. 1.4 Origin Adapted from the Geospatial Centroid’s GitHub Workflow Document, Caitlin Mothes, built out for ROSSyndicate by Katie Willi, B Steele, and Matt Brousil. "],["an-introduction-to-git-and-github.html", "Section 2 An Introduction to Git And GitHub 2.1 Setting up Git and GitHub 2.2 Developing, viewing, and editing repositories 2.3 Making changes to repository visibility 2.4 Additional resources", " Section 2 An Introduction to Git And GitHub Git is a version control system that helps manage projects, especially projects that involve multiple people. GitHub is a hosting service for these Git projects (in GitHub-speak, projects are housed in repositories). Here, we will outline how to set up Git, create your own GitHub account, and connect them to you local R environment to easily publish and share the code you develop. A great article on the benefits of using Git and GitHub has been written by Jennifer Bryan, which you can find here and we highly recommend reading! In fact, Jennifer Bryan has developed what we feel is the best guide for using Git and GitHub out there, Happy Git and GitHub for the useR. So as not to reinvent the wheel, this outline will mostly just direct you to that body of work. 2.1 Setting up Git and GitHub Matt made an excellent YouTube video (also based on Jenny Bryan’s work) on how to set up your own GitHub account, install Git, and allow them to interact with your personal R environment. It’s also a great primer for how Git and GitHub work in R Studio. embed_url(&quot;https://www.youtube.com/watch?v=Y4sOTFV3FAM&amp;t=1s&quot;) %&gt;% use_align(&quot;center&quot;) Key takeaways: Create a GitHub account. (see https://happygitwithr.com/github-acct.html) Install Git on you local computer. (see https://happygitwithr.com/install-git.html) In the R Studio Terminal, connect your GitHub account to your local computer. (see https://happygitwithr.com/hello-git.html) 2.2 Developing, viewing, and editing repositories As mentioned in Matt’s video (~7:53), creating a repository is the first step when starting a brand-new Git project. Essentially, a repository is the main folder in which all the files and code related to a project live. For any repository on GitHub, there are two ways of copying it to your local computer: cloning and forking. 2.2.1 Cloning Cloning a repository maps a local version of that repository to your computer, allowing you to sync both the local (your computer) and remote (what’s on GitHub) versions. This type of copying is useful for workflows where you are the only one working on them. If you clone a repository that you did not create yourself, you will be unable to make changes to the ‘main’ repository hosted on GitHub unless the developer has given your account collaborator approval, but it will allow you to refresh and update what’s on your local computer if changes have been made to what’s on the ‘main’ repository on GitHub. However, for a repository that you have created yourself or are a collaborator on, cloning allows two-way syncing; you are able to make changes to the GitHub repository as well as update what’s on your local computer if the GitHub repository changes. After editing, adding, or removing pieces of the project, you can re-sync those changes by committing and then pushing the project back to GitHub. A commit essentially packages and saves the changes you made; it also allows you to comment on what exactly you changed in the repository. Meanwhile, the push command sends those changes that were packaged by the commmit to GitHub and updates the remote repository on GitHub accordingly. Because cloning creates a direct connection between the GitHub ‘main’ repository and what you’ve cloned to your local computer, you can only push changes to repositories that are located in your own GitHub account, or to repositories that the developer of the repository has explicitly made you a collaborator on. How to clone and update your GitHub: Create a repository on GitHub. (https://happygitwithr.com/new-github-first.html#make-a-repo-on-github-2) Clone the repository to your computer. (see https://happygitwithr.com/rstudio-git-github.html#clone-the-test-github-repository-to-your-computer-via-rstudio) After changes have been made, commit and push them to your GitHub. (see https://happygitwithr.com/rstudio-git-github.html#make-local-changes-save-commit) 2.2.2 Forking Unlike cloning, forking a repository creates a totally separate, parallel copy of a repository that is housed within your own personal GitHub account. Specifically, if you’ve made changes to the code in a forked repository then committed and pushed those changes, it does not automatically make those same changes to the ‘main’ repository that it was forked from on GitHub. Instead, all changes are saved and stored only within the repository hosted within your own GitHub account. Forking essentially creates a line of defense for changes in a workflow that would otherwise disrupt the workflows of other people working within that ‘main’ repository project. For the ‘main’ repository to be updated with the changes you made in your forked version, you must submit a pull request. A pull request is an invitation to the owner of the ‘main’ repository to review the changes you’ve made to the code and potentially merge them into their ‘main’ repository. If the owner/reviewer finds that the code you’ve developed would be a good addition to the workflow and/or that the code has not diverged/disrupted others’ workflows, they can approve the pull request, and the ‘main’ repository will be updated accordingly. Matt and a former grad student, Bryce Pulver, made another excellent video that walks through an example collaborative workflow using forking: embed_url(&quot;https://www.youtube.com/watch?v=a_YY_PIDeq8&quot;) %&gt;% use_align(&quot;center&quot;) How to fork and submit a pull request Fork the repository on GitHub, then clone the forked repository to your local environment. (see https://happygitwithr.com/fork-and-clone.html#fork-and-clone-without-usethis) Create a direct connection with the main repository to keep your personal repository up-to-date. (see https://happygitwithr.com/fork-and-clone.html#fork-and-clone-finish) After changes have been made to the repository on your local computer, commit and push them to the forked repository on your personal GitHub. (see https://happygitwithr.com/rstudio-git-github.html#make-local-changes-save-commit, or Bryce at ~25:45) Go to GitHub in your web browser. Submit a pull request by going to the main repository and selecting and creating a pull request. (Bryce does this at ~31:40) 2.3 Making changes to repository visibility When you create a new repository you can choose whether you’d like its visibility to be public or private. Within the lab our default expectation is public, in order to make our work open and reproducible. However, in certain circumstances repositories will be need to be private or visibility will need to change during the lifecycle of a project. Before you make a change to the visibility of a repository, it’s important to make sure that all of your forks are up to date and to warn any of your collaborators that you are going to make this change. Changing visibility severs connections between all forks and the repository, so this is important to prepare for. 2.3.1 Reconnecting severed repositories You can follow these steps if the connection between your fork and the upstream repository is severed: Rename your fork. This will enable you to have this (old) fork exist at the same time as a new fork in step 2. Create a second fork of the repository. Remove all files from the new fork. Clone your existing fork locally using this command in the command prompt: git clone --bare https://your_original_fork_url. Move into the newly created folder from step 4 using cd your_original_fork_folder. Run the following command. It will push the branch and commit history into the new fork: git push --mirror https://new_fork_url. Instructions adapted from this post and input from B Steele. 2.4 Additional resources Below is a list of additional resources about GitHub and collaborative workflows. • GitHub website (https://docs.github.com/en). • Forked workflows (https://www.atlassian.com/git/tutorials/comparing-workflows/forking-workflow). • Using Git and GitHub for team collaboration (https://medium.com/anne-kerrs-blog/using-git-and-github-for-team-collaboration-e761e7c00281). • Starting a new group project on GitHub (https://www.digitalcrafts.com/blog/learn-how-start-new-group-project-github). "],["workflow-overview.html", "Section 3 Workflow Overview 3.1 Starting a new repository 3.2 Forking and Cloning ROSSyndicate Repositories 3.3 Collaborative Coding 3.4 Addressing merge conflicts", " Section 3 Workflow Overview If you are not already a member of the ROSSyndicate GitHub Organization, please Teams Katie Willi to add you! 3.1 Starting a new repository From the ROSSyndicate Organization landing page, create a new repository (‘repo’) with the appropriate naming convention (short/concise, no spaces). Elect to start the repository with a README.md file and a MIT use license file. After you’ve initialized your repository, go in and edit the third line of the license file to read: ‘ROSS - Radical Open Science Syndicate and all contributors’ Add collaborators with write/push access by going to repo Settings -&gt; Manage Access -&gt; Invite a Collaborator. After you have forked the repository to your account, clone the repo to your local machine (see Forking and Cloning ROSSyndicate Repositories). Once the repo is stored on your local machine create the general architecture for your repository. Create a basic folder structure for your project. Below are some folder types that you may wish to include at the initiation of your project. data: folder containing data sourced throughout the pipeline. For internal project, this should be a SymLink folder to a OneDrive folder (see ‘Data and OneDrive’) . For external projects/deliverables (with datasets that do not trigger large-file errors &gt;100mb), this can be a folder that contains data files that travel with and are tracked by the repository. src: folder containing scripts for the analysis completed in the repository. out/output: folder that contains and tracks output of your pipeline. This could include figures, summary data, etc. Your out/output folder could also live within the data folder, depending on the size of your project output. Populate the README.md with some basic information, including the primary contact for the repo, and what the repo is all about. Also include the following use license blurb: ‘This repository is covered by the MIT use license. We request that all downstream uses of this work be available to the public when possible.’ Populate the .gitignore to ignore any folders or files that should not be tracked. At a minimum, add these to all .gitignore ’s: .Rproj.user .Rhistory .RData .Ruserdata .DS_Store **/.DS_Store Create an R project associated with the repository directory. Commit and push these changes to your forked repository, then create a pull request to your creative partner (see ‘Collaboration workflow overview’). 3.2 Forking and Cloning ROSSyndicate Repositories Go to the ROSSyndicate repo for the project and click “Fork”. Follow the prompts to create a forked repository on your personal GitHub account, which is where you will make changes and edits prior to making a pull request to incorporate your changes to the primary repository. Clone your forked repo to your local machine using the terminal in R or a command-line interpreter. Make sure you have navigated to the directory where you would like the cloned repository to be stored. See this tutorial on command-line basics so you know how to navigate at the terminal. Most folks in the lab create a ‘GitHub’ directory within our Documents folder, but we don’t have any required location for where you store your local repository. Then, clone your repo! You can acquire the ssh_address from your forked repository on the GitHub website. If I were cloning the repo below, I would replace ssh_address with git@github.com:steeleb/ROSSyndicate_best_practices. git clone ssh_address 3.3 Collaborative Coding (Note, the workflow for this is slightly different if you are working in feature branches. Please see that section if you are working in feature branches.) Always start your day by fetching upstream changes from the primary repository by typing the following in the terminal. This is part of the way we avoid merge conflicts down the road. git fetch upstream Work on your local repo, commit, edit, commit, edit, commit. See ‘Commit history best practices’ for advice on great commits. Commits are made in your terminal with the following commands: git status This will show you what files have changes, and which are staged for your commit. Now, stage the files for your commit. git add filename.ext filename2.ext Check to see that you’ve staged the correct files for your commit with git status It will look like this: If you are happy with the files listed under ‘Changes to be committed’, make a commit! git commit -m “insert commit message here” Push your commit history to your forked repository. git push origin main Submit a pull request to your internal creative partner or engage other members of your project team through a pull request. See ‘Pull Request best practices’ below for details. Navigate to your forked repository. Confirm that your branch is up-to-date with the upstream repo by clicking on ‘Sync fork’. Then, click ‘Contribute’ and ‘Open pull request’. Craft and submit your PR! Add a title and description of your pull request, making sure your PR conforms to the ‘Pull Request best practices’ recommendations. Pull request descriptions and comments are written in Markdown, so take advantage of embedding images, emojis, text blocks, etc. You can also associate specific commits and issues with markdown text using the hash ‘#’. Check “Allow edits from maintainers” to allow other collaborators to make changes to your pull request and in the right hand side, assign a reviewer (in this case, your internal creative partner or whomever you’d like to engage in this pull request). Click “Create Pull Request”. Internal creative partner (or engaged individuals) reviews the pull request according to ‘Pull Request Review best practices’ following the process outlined in GitHub’s How to Review Pull Requests. If no changes are required, the reviewer will merge the branch, but if any comments have been made on the pull request, leave the merging to the PR author. 3.4 Addressing merge conflicts There are multiple ways to deal with merge conflicts. You can address them within the GitHub conflict editor. Alternatively, you can use the command-line method is described here, but this method requires some advanced skill and understanding of command line git. If you’re using VSCode, and have the ‘GitHub Pull Requests and Issues’ extension installed, this is a very user-friendly way to deal with conflicts, too. "],["collaborative-coding-and-workflow-best-practices.html", "Section 4 Collaborative Coding and Workflow Best Practices 4.1 ROSSyndicate style guide 4.2 Commit history best practices 4.3 Pull request best practices 4.4 Pull request review best practices 4.5 Types of code review and engaging your internal creative partner 4.6 Additional Reading", " Section 4 Collaborative Coding and Workflow Best Practices Some sections of this document borrow liberally from the additional resources listed at the end of the document. 4.1 ROSSyndicate style guide ROSSyndicate aims to incorporate the {tidyverse} style guide for our codebase. Be sure to standardize file/variable/function style and naming conventions at the beginning of the project. While this is a somewhat aspirational goal, understanding style guides will help us all become better programmers. In particular, familiarize yourself with the sections under ‘Analyses’. We suggest re-reviewing the {tidyverse} style guide a few times per year just to be reminded of best practices. Some general guidelines: Use {tidyverse} functions over {base} functions, except in package development. Use custom functions over for-loops. If you use the same code chunk three or more times, make it a function and use purrr::map() (or any map() sibling). Use the {magrittr} pipe (%&gt;%) over native pipe (|&gt;). Always include comprehensive documentation for your code. 4.1.1 Using the ‘Styler’ Addin To facilitate adoption of the tidyverse style guide, please install the RStudio Addin ‘Styler’. To do this, click ‘Addins’ at the top of the RStudio window, then select ‘Set style’. This will prompt you to install the necessary packages and then define the style transformer. The default is {tidyverse}, so all you have to do is press ‘OK’. As you work on scripts within your repository, select ‘Style active file’. This will modify the file to conform to {tidyverse} style. Be sure to commit these changes. Styler will not tell you what has changed, all you will see is the following message in your console: While using {styler} will not correct all style choices, it will fix spacing, assignments, and tabs automatically. 4.1.2 Fonts, colors, and .css [in development!] In order to reduce any bandwidth necessary and get the most out of your coding deep work time, we suggest using a few hacks: Use the style_snippets in this repo 4.1.3 More on literate code… One aspect of literate code relative to style is that we expect code and code output to be readable in [nearly] any format. This means whether you are using the Visual editor in RStudio, you’re reading a rendered .html or if you’re viewing an .Rmd on the web or in the Source editor of R Studio. To accomplish this, we suggest line character length no greater than 80. If you are in an .Rmd file, just add the last code chunk from the style_snippets/markdown_helpers.Rmd, save your file, and run the chunk. This will reformat all lines of text so that they are on new lines every 80 characters. If you are in .R or a .md document, use the page-width indicator (the light vertical blue-grey line on the right of your editor box) in the Source editor in RStudio to determine when to create a new line in your text block. Here’s an example of run-on lines: And what it should look like: 4.2 Commit history best practices Always check your forked repository against the upstream repository before beginning your coding session and certainly before making any changes to the code base using the terminal command git fetch upstream . Think about using your commit history as bullet-point summaries of your future pull request. If you need to go back and make some change in your commit history, you’ll want to have a good idea of where that happened. Commits should apply to a singular file or multiple specifically-selected and related files. In general, this means avoid git add . unless all the changed files are related - like a .Rmd and the rendered .html. Commits should briefly summarize the changes (e.g., ‘work on X’, NOT ‘end of day commit’). Commits should describe WHAT and WHY you’ve made changes/added to your code. Always commit your changes to your local repository or branch and push to GitHub at the end of your coding session as backup policy (i.e., don’t leave changes un-tracked when leaving work), ideally this also will result in a pull request to your internal creative partner. It’s always easier to write a pull request when your work is fresh in your mind. 4.2.1 Adding a co-author to your commit We strive to always attribute code as we develop our code-base. If you borrow code from someone else’s repository, or if someone has shared a code chunk with you, add them as a co-author on your commit. To add a co-author, you need to know the person’s GitHub user name or their name and email (if they are not on GitHub). We suggest making a commit prior to adding any files that originated from another coder or entering your collaborator’s code chunk. This way, it is very clear what the co-author’s contribution was. So, commit, add the code from your collaborator or another coder, then commit those changes with them listed as a co-author: git commit -m ’Adding function from George Co-authored-by: George Friend &lt;gfriend@users.github.com&gt;’ Note that you can use the contributor’s GitHub user name (in the example above: ‘gfriend’) 4.3 Pull request best practices The only time you should merge your own pull request is at the initiation of your repo, as described in ‘Create a New Project’. Pull request should happen frequently, ideally at the end of each work session. Pull requests should be 400 lines of code (LOC) or fewer. Use the title of the pull request to help orient your creative partner. Clearly identify what you need in the pull request review. An example of a good pull request, that closes an issue created in your GitHub Project (we’re assuming issue #7 is ‘targets-ify the repo’), and makes a specific request of the reviewer: Title: apply {targets} to workflow Primary files for review are the _targets.R files in the main directory and in each subdirectory in the numbered directory step-by-step process. Please check for functionality by cloning and running tar_visnetwork() for each _targets.R file. Closes #7 If you have a large pull request, i.e. at the beginning of code development that is more than 400 LOC, write a useful description alongside your pull request that introduces the repo’s concepts (&lt;awesome-workflow&gt;) and highlights specific files (XXX, ZZZ) or commits (5e6b825): Purpose: initialize development of YYY repo and start buildout of &lt;awesome-workflow&gt; Start of YYY repository. Please review diff for XXX and ZZZ files. Otherwise, please do a high-level check of the repository as of the latest commit (5e6b825) for general workflow. 4.4 Pull request review best practices Activate notifications for pull requests and comments so you know when a pull request has been submitted for your review. If you cannot review the pull request that has been assigned to you before the end of the next work day, contact the person who has made the request via Teams to let them know when you *can* review the PR. The person who has made the review can then assign the PR to another person if the review is time sensitive. If possible, cease work on this repo until the PR has been reviewed. Every additional git push origin main that you make after you submit a PR and before it is reviewed and merged will be added into the current PR. If you MUST continue work on the repo while the PR is being reviewed, do so in a Feature Branch. If the PR is not clear or does not contain specific asks, DO NOT REVIEW OR MERGE, message the pull request creator on Teams and request them to edit the detail of the PR to reflect the best practices outlined above. “Before you even look at the code [for review], focus on understanding what was changed, why it was changed, and how it was changed.” – Leone Perdigão, Medium Be kind and thoughtful with your comments. We encourage the use of ‘conventional comments’ labels found here. This allows you to clearly engage with the developer on the other end of the PR and offer actionable comments. As a reviewer, you should not spend longer than one hour per session in active code review and never review more than 500 LOC per hour (SmartBear). Ideally, code review of a PR takes around 30-45 minutes when a project is in active development and follows the best practices outlined above. What to look for in code review (via Google). Note that you do not need to check for all of these during a review of code as some are more applicable at the beginning of codebase development and others are more applicable to review that occurs closer to codebase deployment. Design: Is the code well-designed and appropriate for your system? Functionality: Does the code behave as the author likely intended? Is the way the code behaves good for its users? Complexity: Could the code be made simpler? Would another developer be able to easily understand and use this code when they come across it in the future? Tests: Does the code have correct and well-designed automated tests? We do not currently employ tests in our code. We will update information on this as we develop ways to include tests in our codebase. Naming: Did the developer choose clear names for variables, classes, methods, etc.? Do they follow the tidyverse style guides? Comments: Are the comments clear and useful? Style: Does the code follow our style guides? Documentation: Did the developer also update relevant documentation? This is especially applicable to the README.md files in your repository and directories. 4.5 Types of code review and engaging your internal creative partner Code review is a skill that is learned and developed. Ideally, each member of the lab is performing some sort of code review on a weekly basis to grow skill in this realm. [Some] types of code review: Peer-to-peer code review 30-minute session explaining alpha-level code (code in baby development that might be in a branch or generally offline from the ROSSyndicate organization) line-by-line code walkthrough meeting 30-minute to 1 hour ‘over-the-shoulder’ code observation and review Code review via Pull Request:  ‘process review’: This type of review is only suggested for early feature development OR early repository development. Do the decisions and workflow order make sense? Big-picture review only and/or suggestions to reduce processing time welcome.  This type of review is either preceded or followed by a peer-to-peer code walkthrough and explanation. ‘general code review’: iCP reviews code changes according to usual pull request review best practices. This will be the most common form of code review via pull request ’ready for release: If a project features end-user reproducibility, i.e. Shiny app, published workflow, etc., the iCP should run this code in a fresh environment to make sure it runs properly and that dependencies are satisfied before pushing for public/partner use. Ideally, code developers are engaging in some kind of code review on a daily to weekly basis 4.6 Additional Reading The Best Way to Do a Code Review on GitHub, LinearB GitHub Code Review Best Practices, Mergify [How To] Commit and Code Review on GitHub, Birkhoff Tech Blog Pull Request Best Practices, Medium The (written) unwritten guide to pull requests, Atlassian Best Practices for Code Review, SmartBear "],["project-management-and-tracking-with-github-projects.html", "Section 5 Project Management and Tracking with GitHub Projects 5.1 Github Project best practices 5.2 GitHub Project additional reading", " Section 5 Project Management and Tracking with GitHub Projects We use GitHub Projects to track progress and assign tasks to collaborators. A project is a user interface that integrates with your issues and pull requests on GitHub to help you plan and manage your work effectively. For Matt R, this will be where he can go to see quickly where a project is at on its deliverables, so it is important to keep it up to date. Projects have immense flexibility and can be tailored to fit the needs of any project. 5.1 Github Project best practices Frame the Project tasks around the statement of work/objectives of the project. Take advantage of tags to easily filter and peruse different types of tasks. Break down large tasks into GitHub issues that can [ideally] be completed in a single pull request. See also GitHub’s document of best practices. 5.2 GitHub Project additional reading GitHub Projects: https://docs.github.com/en/issues/planning-and-tracking-with-projects/learning-about-projects/about-projects Example Project: https://github.com/orgs/ROSSyndicate/projects/4 "],["git-hub-feature-branches.html", "Section 6 Git Hub Feature Branches 6.1 When to work in a feature branch 6.2 How to create a feature branch 6.3 Feature branch best practices", " Section 6 Git Hub Feature Branches 6.1 When to work in a feature branch Working in feature branches is similar to working in a forked repository, except you have two parallel workflows with varying commit history in the same [forked] repository. It is essential, if you are using branches, that you are working on a very specific issue or feature (hence the name) that is independent of others’ work on the repo – or even your other branches! IMPORTANT: Try not to leave your branches unmerged for long periods of time if other folks are working in the same repository to help alleviate merge conflicts. Working in a feature branch is also a way to continue working on a codebase as other code is in review. 6.2 How to create a feature branch Make sure the forked main (‘origin main’) is updated with upstream changes and all of your current work has been committed to the main branch, then start a new branch: git checkout -b new-branch-name Branch starts with a copy of the branch you were on. You can now make edits in this branch and add/commit as usual. If you switch back to the main branch using git checkout main, all of your commit history will revert to that on the main branch (aka, it will look like all of your work disappeared). You can either push your branch up to your origin remote (your forked repo) using: git push -u origin new-branch-name Or you can go back to your main branch and merge your edits from the feature branch: git checkout main git merge new-branch-name You can also submit a pull request from your feature branch the same way you would if you were making a pull request from your main branch. Just make sure you have selected the proper branch from your forked repo. In general, we only have a single branch (‘main’) in use at our primary ROSSyndicate repository. 6.3 Feature branch best practices Keep your fork up to date regularly!! COMMUNICATE. Each team member works on separate files/features (no overlap, feature collaboration can take place in the pull requests). "],["using-ai-integrations.html", "Section 7 Using AI Integrations 7.1 Using GitHub CoPilot in VSCode + RStudio 7.2 Accessing GitHub CoPilot as a student/teacher/researcher 7.3 Downloading VSCode and installing the GitHub CoPilot extension 7.4 Setting up AutoSave in VSCode + RStudio", " Section 7 Using AI Integrations The following section is a modification of a Twitter thread written by Santiago Valdarrama (@Santiago). In the ROSSyndicate, we are actively trying to use some of the large language models to make our jobs easier/more efficient. AI models can be valuable tools in code development if used effectively. These models can understand and generate human-like text, offering assistance in writing and suggesting code snippets. However, it’s important to note that while they enhance productivity, AI models are currently not a replacement for coding; they should act only as supportive aids. Never take the response of an AI model at face value. AI models are flawed and make mistakes often. Even so, they have the potential of saving you substantial time and can give you a massive boost. With that, here is a list of ways in which AI models can be effectively used to help you on your coding journey. Explaining code. AI model explanations can be very detailed. Dropping convoluted or confusing code into an AI model (plus a request to explain it) can be much quicker than trying to figure out code on your own. Improving existing code. Asking an AI model to improve existing code by dropping it into the chat-bot plus describing what you want it to accomplish will often result in a response that includes working (or, at least close to working), modified code. Rewriting code with a different style. For example, it can be great for re-writing code written in base R into {tidyverse}-style code. AI models will not only give you the updated code, they will also explain the reason for the changes they make. Simplifying code. Asking AI models to simplify complex code can result in a much more compact version of the original code. The output will also provide an explanation for the changes, as well as whether the simpler version is as efficient as the original code. Debugging. If you are having a hard time finding the source of an error in your code, AI models are a great solution. Just provide it the code, and it will often find it and fix it for you. Workflow development. Use AI models to kick off the structure of anything new you want to write. Writing test cases. When writing functions, you can use AI models to generate test cases for you to use in testing whether a function is working the way you want it to. Exploring alternatives. Asking an AI model what the most efficient way to perform a coding action might teach you new ways of writing more efficient and sophisticated code. Translating code. Anytime you want to translate some code from one language to another, ask an AI model to help you. Writing documentation. Ask an AI model to write the documentation for a piece of code, and it usually does a pretty great job. It will often even include usage examples as part of the documentation. Integrating AI into your workflows: Below is one of the most sustainable ways to integrate the CoPilot LLM into your workflow. We encourage all members of the lab to sign up for (and use!) any of the following sites: ChatGPT GitHub CoPilot Perplexity AI 7.1 Using GitHub CoPilot in VSCode + RStudio VSCode also has an extension for GitHub CoPilot, it will auto complete your comments and code in nearly any programming language. By having open both VSCode and RStudio and enabling auto-save in both programs, you can hop between the two programs and code will update between the two as you switch. Big thanks to Sam Struthers for figuring out this integrated workflow. 7.2 Accessing GitHub CoPilot as a student/teacher/researcher Access to GitHub CoPilot is currently free to students/teachers/researchers. Follow the instructions here to submit an application to gain access to the program: https://education.github.com/discount_requests/pack_application. A screenshot of your ‘employee information’ from the human resources portal will be sufficient information to verify your employment. GitHub usually will give you access to the educational portal within 1 business day. 7.3 Downloading VSCode and installing the GitHub CoPilot extension Download VSCode here: https://code.visualstudio.com/. Set up the GitHub and GitHub CoPilot extensions by adding them via the ‘Marketplace’. You can bring up the Extensions from the menu bar under ‘Code’, ‘Settings’, ‘Extensions’. You’ll want to install and activate ‘GitHub Pull Requests and Issues’ as well as ‘GitHub Copilot’. You will be directed to Sign In using your GitHub credentials. You may need to elect to ‘enable’ Copilot. 7.4 Setting up AutoSave in VSCode + RStudio In RStudio, open the Preferences pane from the menu bar under ‘RStudio’, then find the following pane: Set up Auto-save by checking the ‘Automatically save when editor loses focus’ and ‘Apply’. In VSCode, select ‘Code’, ‘Settings’, then ‘Settings’ again from the menu bar to bring up the Settings tab in your console. Then type in ‘auto save’ in the top search bar, and select ‘onFocusChange’ for the ‘Files: Auto Save’ Setting. Now you can switch between programs, using VSCode and CoPilot to draft code and comment chunks, then further editing/running in RStudio. "],["archiving-data-and-code.html", "Section 8 Archiving Data and Code 8.1 Choosing a data repository 8.2 Publishing code to Zenodo 8.3 Philosophical and Ethical Considerations", " Section 8 Archiving Data and Code We archive data and code through a number of pathways. Archiving our code and data make downstream analyses reproducible, is a great way to share data, and also provide the ability for citation through a stable DOI. Generally speaking, it’s good practice to get a DOI for any data before sharing it, as this helps with data provenance, additionally, most journals now require that all code and data are archived using a DOI. The flowchart below will guide you in the preferred method of archiving. 8.1 Choosing a data repository The decision of which data repository to use is less straight forward. There are benefits and trade-offs to each. The Environmental Data Initiative (EDI) requires a significant amount of metadata in the form of an EML (ecological metadata language) file that describes the data and is the best for .csv files, however, this metadata makes the data more usable by downstream users. HydroShare features a less rigid metadata requirements, and is better suited for rasterized data and non-.csv file types. FigShare requires no metadata and because of that there are numerous data products with no metadata that are stored on FigShare. Generally speaking, all data that is published from this lab should have thorough metadata regardless of the metadata requirements of the data repository chosen. For one-time uploads to EDI, you can use the ezEML interface of EDI to create and format your EML. If you plan to create many data packages, make an EDI account so that you can upload yourself. If you have a dataset that will be updated frequently, use of the R package {EMLassemblyline} is useful. If creating metadata for other data repositories, review the content of the EML and include as much of that information in your metadata as possible. 8.2 Publishing code to Zenodo When delivering code or repositories to our stakeholders or other researchers, it needs to be citable. A citable repository has a stable DOI (digital object identifier) and complete metadata. We use the GitHub integration in Zenodo to publish repositories. When set up correctly, a GitHub ‘release’ will push all code to Zenodo automatically. All future ‘releases’ will be associated with the original DOI as well as a new DOI specific to that release. In general, the GitHub -&gt; Zenodo pathway is best for software (code) and not necessarily for data. Data publishing should be completed at EDI, CUAHSI, or FigShare. Derived data is okay in the GitHub -&gt; Zenodo pathway, just remember, this pathway is meant to be for CODE and SOFTWARE, not necessarily for data (though you can publish data to Zenodo). As mentioned before, we only publish repositories that have been consistently reviewed internally. Ideally your repository has been reviewed by an external collaborator or by another ROSSyndicate member. Remember, by pushing this repository to Zenodo, you are creating a PERMANENT archive of its contents, for better or worse. 8.2.1 Creating a Zenodo account The easiest way to create a Zenodo account is to do so with your GitHub credentials. You will need to link these two accounts in order to fully take advantage of the GitHub/Zenodo integration. 8.2.2 Preparing to publish In order to have a meaningful repository release, you need to provide some metadata to Zenodo. While there are no requirements for publishing to Zenodo, it is a best practice to include robust metadata alongside your code release. A metadata template can be found in the helpful_docs folder of this repository. Once you’ve compiled this information, send it to Katie Willi or Anika Pyle for proofreading. Release Title You can use the repository name, but that’s not going to be helpful for anyone except you! Someone reading the Release Title should be able to discern what the repository does. You can explain the ‘how’ in the description. Author list Prepare your author list by looking at the contributors to the repo. All contributors to the repo should be listed as an author. You will need each author’s full name, institution, and their ORCID. Description This is the big lift in creating the metadata. Consider this similar to the introduction/background and methods section of a scientific paper. In the description of your repository, add information about the following things: General code description - the ‘how’ of your repository Methodology (summary only) - specifically if there are multiple steps to an analysis it is helpful to provide a methodology. If the repository hosts a very complicated analysis, consider only a condensed version of methods here, but add a markdown document with the complete methodology in your repository. If you are including data: data lineage (aka, where the data came from) explanation of how the data have been manipulated if they are derived (this is related to methods, so use your best judgement). If the analysis is specific to a certain locale, state and describe the locale. Maintenance and maintenance interval. ‘Ongoing’ or ‘completed’. If ‘ongoing’, state the approximate interval that the repository will be updated. If this is an update to a previous release, state the changes from the previous version. Generally speaking, much of this information should already exist in your repository README or Methods document and cutting/pasting is absolutely okay. This section may be lengthy and that is okay! Version For one-time releases (like preparing a repository for a paper), using a version-number is perfect (i.e. v1, v2, etc). For repositories that are updated frequently, consider using a version-year-number (i.e. v2022-1, v2023-1, v2023-2, etc). Keywords Generally speaking, keywords help others find your code or research. It is not required. Keywords should be meaningful and specific and can include places, methods, etc. It is common to use controlled vocabulary for keywords, like the LTER controlled vocabulary for ecology research. License All repositories that are published are considered open access. Because our repository best practices include using an MIT use license, the LICENSE.txt file will override this option. Funding If there is applicable funding sources for this repository, you should list them. You will need the funder name and the grant number. No other fields are required. 8.2.3 Linking a repository to Zenodo To activate the GitHub/Zenodo integration for a given repository, navigate to the GitHub account page by clicking the dropdown menu in the top right of the window. From here, you simply follow the directions! 8.2.4 Completing the metadata information To complete the entry of your metadata information and formalize the publishing of your repository, you’ll need to edit the Zenodo upload. Do this by clicking on “My Dashboard”. Here, you can click on the released repository: And then click the ‘Edit’ button for the associated upload. Here, you can enter in all information from your Zenodo metadata document. When you’re ready, press ‘Save’ and then ‘Publish’. 8.2.5 Add the DOI badge to your repository Now that you’ve minted a sweet sweet DOI for your repository, you should go ahead and add the DOI badge to your repository’s README file. On the right hand side of your Zenodo release landing page (formatted as ‘https://zenodo.org/record/*DOInumber*’ on your browser), click on the DOI badge: This will pop up a number of embedding formats. Copy and paste the one for ‘Markdown’. At this point, go to the GitHub repository and add the badge at the top of the primary README file within the GitHub user interface on the web. This addition to the repository does not need to be reviewed. 8.3 Philosophical and Ethical Considerations As a general philosophy we acknowledge that there is bias in every data set and in the decisions we make along the way as we code. These biases may be geographical (data are only from one location), contextual (data are only from a single source), or implicit (data have been altered for a specific purpose). As you prepare your data and metadata, it is important to think about the biases we contribute to a given dataset. To familiarize yourself with bias that we introduce into our data and analyses, here are some resources: “Biases in lake water quality sampling and implications for macroscale research” [@stanley2019] Review the questions in this datasheet created by our colleagues in the Atmospheric Sciences department when creating metadata and general documentation A deep dive into bias in AI/ML: “Why we need to focus on developing ethical, responsible, and trustworthy artificial intelligence approaches for environmental science” [@mcgovern2022] "],["python-integration-in-rstudio.html", "Section 9 Python Integration in RStudio 9.1 Setting up a python virtual environment", " Section 9 Python Integration in RStudio 9.1 Setting up a python virtual environment If you need python modules (the equivalent of R packages) that are not included in the python base modules, you will need to set up a virtual environment for reproducibility. In order to do this successfully, you’ll need to edit the pySetup.R script so that it suits your needs. An example of this script can be found at python_setup_helps/pySetup.R. Best practices note: you should not track your virtual environment folder in GitHub. Add the folder extension to your .gitignore file and confirm that it is not tracking the folder. 9.1.1 Python setup code snippet This snippet checks to see if the ‘env’ folder exists - this is where the virtual environment is set up, and if it is not, it runs the pySetup.R file. If the ‘env’ folder exists, we use the {reticulate} function use_condaenv() to read the virtual environment you already created in the pySetup.R script. library(reticulate) if (!dir.exists(&quot;env&quot;)) { source(&quot;pySetup.R&quot;) } else { use_condaenv(file.path(getwd(), &quot;env&quot;)) } 9.1.2 Editing the pySetup.R file The things you may have to edit are the lines for the python modules you want to install in your python virtual environment (on line 6), and the version of python you want to use. The script defaults to python version 3.8 as it seems to be the most stable for the earthengine-api module, but any version can be specified. "],["workflow-management-with-targets.html", "Section 10 Workflow Management with {targets} 10.1 Using the {targets} package to automate your analyses", " Section 10 Workflow Management with {targets} 10.1 Using the {targets} package to automate your analyses Data science workflows will often grow beyond a single script and can rapidly become unwieldy when many steps, datasets, and files are involved. This is both a challenge to keep track of (i.e., which scripts to run first, second, etc.) and a challenge to communicate to other users in the world of open science. Workflow management software is a solution to this that the ROSSyndicate has embraced. Specifically, we use the {targets} R package. {targets} in particular allows you to use R to build out a pipeline (used interchangeably with workflow here) which the software automatically tracks so that it “knows” which steps have been run, which have failed, and which need to be re-run. Your entire analytical pipeline or workflow can be rerun with just the command tar_make() and you can visualize the connections between objects and functions in your pipeline with tar_visnetwork(). One standout strength of this workflow management architecture is that the software “knows” which downstream steps are affected by a change in the upstream code or datasets. This means that you as a human have no need to manually re-run scripts that are outdated by changes in code; {targets} knows which are affected, can tell you, and automatically re-run them. Not every analysis is a perfect fit for a {targets} workflow, however. Very small analyses may not be worth the time needed to invest to build out the structure of the pipeline. Workflows that are often re-run with updated datasets or which can perform many parallel iterations are especially good candidates for the {targets} pipeline style. Because the ROSSyndicate values literate code and {targets} does not render and track the output of Markdown or Quarto documents, there are additional steps to creating easily understandable and literate code within the {targets} framework. In short, it involves using the tar_render() function to create a bookdown from Markdown documents as one of the final steps in the workflow. In that bookdown, you would integrate our practice of literate coding. We will be developing more framework around this as time passes. We won’t explain how to use {targets} in detail here, but good resources are readily available. The {targets} R package user manual is a great resource written by the author of the {targets} package. Matt Brousil from the ROSSyndicate has also written a short example of building an analysis with the package in Targets for Ecologists and created an instructional video. 10.1.1 Hot tips and tricks Sourcing scripts for functions in {targets} pipelines The _targets.R script is an essential piece of a {targets} pipeline. This file is also where functions that define workflow steps (“targets”) are sourced from other scripts. {targets} provides a built-in function to do this, tar_source(). For example: tar_source(\"src/custom_functions.R\") Package use in a {targets} pipeline You might find it unclear how to require certain packages when writing code for a {targets} workflow. The ideal situation is that you use the tar_option_set() function near the top of your _targets.R script and provide the packages argument a character vector of package names that should be loaded for every target in the workflow. This often might just be tar_option_set(packages = c(\"tidyverse\")). Then, you can change this default for specific targets within the respective targets’s tar_target() function like this: tar_target(packages = c(\"tidyverse\", \"lubridate\")). Note that you’ll need to still repeat any packages you’ve already listed in tar_option_set() when changing the packages argument in tar_target(). Reading external files into {targets} pipelines When building your {targets} pipeline you may wonder how best to read in data and have the pipeline track the existence of input files. The {tarchetypes} package provides some handy shortcut functions for this: tar_file() and tar_file_read(). The first, tar_file(), identifies that a pipeline target is a dynamic file based on a path provided to the command argument. By contrast, tar_file_read() will create two pipeline targets: one tracking the file path provided, and a second reading in the file given the instructions you provide to the read argument. For instance, if you have a .yml file that you want to track for changes, you can use the tar_file() method for tracking. If you have a file (for example a .csv) that is output from a target that you want to use later, your target that creates that .csv would return the filepath of that .csv. You could then use tar_read() to store the data in that .csv as a target and track the file (as your filepath). Using the ‘branching’ function in {targets} There are times where your workflow will benefit from tracking specific inputs into a target. For instance, in the Landsat acquisition workflow, we repeat a function over every single WRS path-row over the United States. Sometimes this acquisition fails because it takes a long time to execute. Deploying branches requires two steps: first, you create a list to iterate (or branch) on - in this case, a list of path-rows; then you map that list over the function that relies on the path-row. Your {targets} list might look something like this: list( # get WRS tiles - this target returns a list of path-rows tar_target( name = WRS_tiles, command = get_WRS_tiles(WRS_detection_method, yml_file), packages = c(&quot;readr&quot;, &quot;sf&quot;) ), # run the landsat pull as function per path-row tar_target( name = eeRun, command = { ref_pull_457_DSWE1 ref_pull_89_DSWE1 run_GEE_per_tile(WRS_tiles) }, pattern = map(WRS_tiles), # this is where the magic happens! packages = &quot;reticulate&quot; ) ) Using python scripts in {targets} The {targets} package was designed to work for R workflows only, which means for our mixed-language workflows, we have to get creative. Again, {targets} will not render AND track the output of a .Rmd file, which means all of our precious .Rmd must be translated into .R and .py scripts in order to work in {targets}. We employ the use of source_python() from the {reticulate} package. You can use this in the same way you might use tar_source() to track all of the functions you call in your pipeline. {targets} will track all of the functions in your .py file as individual targets. To run a python script as a target, you have to nest it in an R function. For instance, our run_GEE_per_tile() from above is just this wrapper to: run_GEE_per_tile &lt;- function(WRS_tile) { tile &lt;- WRS_tile # store the tile from the list write_lines(tile, &#39;data_acquisition/out/current_tile.txt&#39;, sep = &#39;&#39;) # save it as a generic file source_python(&#39;data_acquisition/src/runGEEperTile.py&#39;) # this script actually *reads* the generic file above as one of the first steps } Note, {targets} does not track any of the functions called in the source_python() line like it does in an .R script - you must list all of the functions that are needed in your script in the command list so that {targets} knows you need those functions to run the code. Additionally, targets does not actively track any output from source_python(), so again, you’ll have to get creative! Rendering a Bookdown in {targets} [placeholder] 10.1.2 Resources Below are links to resources to help you learn more about {targets} and implementing workflow management software: The {targets} R package user manual + Improving ecological data science with workflow management software + Targets for Ecologists "],["code-documentation-best-practices.html", "Section 11 Code Documentation Best Practices 11.1 Repository documentation 11.2 Code Documentation 11.3 Function Documentation 11.4 Bookdown Documentation", " Section 11 Code Documentation Best Practices Repository, code, and function documentation are all integral parts of making code understandable to those reviewing your code and other end users. 11.1 Repository documentation When you create a repository, you create a README.md file, but what goes in there?! Here are a few basics that you can use to populate this README file so that others know what your repo does and who to ask questions about it! A few sentences about what the repo does. Include where the data originate (data provenance) and indicate whether or not it uses a symlink folder. If there is an end product (say, a Shiny app), state what that is - and if it is deployed, link it to the README. A primary contact for the repo. If you include your email, spell out “dot” and “at” so that you don’t get spammed. A brief overview of the repo architecture. An onlooker might not know what src is, so and overview for a repo that has a src and data folder might look like this: Folder Descriptions: src: this folder contains all scripts for this repo data: this folder is untracked in git, and must be set up using a symlink for the code to function. Contact XXX for the link to the folder. 11.2 Code Documentation Code documentation takes two primary forms when you are working in an .Rmd, .Qmd, or .ipynb: Text outside of code chunks that gives an overview of the script (which may appear at the top of a script as a chunk that might be called ‘Purpose’ or ‘Overview’). In general, any person should be able to read the text (or headers!) that are outside of the code chunks and know what the script does without ever looking at the code chunks themselves. Commented text inside the code chunks that gives explicit/in depth descriptions about what you are doing. This is information that might be specific to the workflow, but may not be necessary to understand the function of a chunk - like: #using a for-loop here instead of a funciton because… blah blah Code documentation in .R and .py files is limited to commented text only. If there is a workflow that uses .R and .py files (like, a targets workflow), it is expected that the code is commented AND there is a bookdown or markdown-type file that is rendered alongside the workflow that describes it. 11.3 Function Documentation Functions should be documented using formal roxygen-style tags (for R scripts) and docstring-style tags (for Python). This is particularly important for {targets} workflows, but should become common place in any non-literate code we create. This type of documentation is not required for functions that are in a linear workflow, like those in a literate script (.Rmd, .Qmd, .ipynb). 11.3.1 Roxygen-style documentation Roxygen is a formal way of describing a function. It is often used in package development as a way to populate the --help command for a function. We use the style of Roxygen to add documentation to our code. Here is an example of using Roxygen style to add documentation: #&#39; Function to check for and install (but not load) packages. #&#39; This is likely to be used for most {targets} pipelines. #&#39; #&#39; @param x a package name, in quotations #&#39; @returns text string to indicate whether a package was #&#39; installed, or if it was already installed. #&#39; #&#39; @seealso [package_loader()] #&#39; #&#39; package_installer &lt;- function(x) { if (x %in% installed.packages()) { print(paste0(&#39;{&#39;, x ,&#39;} package is already installed.&#39;)) } else { install.packages(x) print(paste0(&#39;{&#39;, x ,&#39;} package has been installed.&#39;)) } } Read more about Roxygen documentation here. 11.3.2 docstring Documentation Docstring is a simple, no-frills way to add documentation to Python scripts. While there are many other documentation methods that are more similar to Roxygen, this is the one that is the most straightforward for our purposes. def csv_to_eeFeat(df, proj): &quot;&quot;&quot;Function to create an eeFeature from location info Args: df: point locations .csv file with Latitude and Longitude proj: CRS projection of the points Returns: ee.FeatureCollection of the points &quot;&quot;&quot; features=[] for i in range(df.shape[0]): x,y = df.Longitude[i],df.Latitude[i] latlong =[x,y] loc_properties = {&#39;system:index&#39;:str(df.id[i]), &#39;id&#39;:str(df.id[i])} g=ee.Geometry.Point(latlong, proj) feature = ee.Feature(g, loc_properties) features.append(feature) ee_object = ee.FeatureCollection(features) return ee_object Docstring documentation is particularily easy to create in VS Code editor using the extension ‘autoDocstring’. This will automatically populate the ‘Args’ and ‘Returns’ fields including the ‘Args’ variable names. 11.4 Bookdown Documentation We will often use {bookdown} to document large projects, especially when using {targets}. {bookdown} is a package that allows you to create a book-like document and deploy as a github.io website, if desired. You can also render to .pdfs. 11.4.1 General style guidance Keep your repository tidy by putting your bookdown .Rmd files, index.Rmd, and associated .yml files into their own directory called “bookdown”. The output directory should be set to “docs” in the _bookdown.yml file. See this example of .yml content: delete_merged_file: true output_dir: &quot;../docs&quot; language: ui: chapter_name: &quot;Section &quot; Note that the output_dir is set in reference to where the bookdown is being rendered (in the “bookdown/” directory). Keep your headers to 3 subheadings (“###’) or fewer. "],["flex-your-github-skills-by-updating-our-website.html", "Section 12 Flex your GitHub skills… by updating our website! 12.1 Directions:", " Section 12 Flex your GitHub skills… by updating our website! As a member of our lab, we would like you to update our “People” page to include you! Here we will lay out the steps for you to fork our lab website repository, add yourself to our “People” page, and lastly, send a pull request to the ROSSyndicate to make these changes on the official website. This is a great chance for you to employ those best practices learned in this document. 12.1 Directions: • Fork the lab’s website repository. • Once in your own R environment, create a copy of Matt’s bio folder (found in content/authors/); rename to your first name. • Update _index.md to reflect your own bio. • Replace avatar.jpg with your own photo; name it avatar.jpg. • Commit the addition of you bio, then push the branch to your forked repository. • In GitHub, submit a pull request with your suggested changes (i.e., including you!) to the website, following the best practices guide. Assign the review to Katie Willi, the lab and data manager. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
